{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3wJhpcpoCOa"
   },
   "source": [
    "# Donut Fine-tuning\n",
    "\n",
    "El presente notebook es el proceso de fine-tuning para [DoNut-base](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2&ved=2ahUKEwjMh4O54vGNAxVsIrkGHQznKskQFnoECBcQAQ&usg=AOvVaw1uKtlO2jgCL6oC_haM4FIB)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqdq_7b14tMU",
    "outputId": "9d7e3bc5-96a4-47ed-908b-44e0c1cd5fe8",
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:01.334936Z",
     "start_time": "2025-06-15T22:52:00.367730Z"
    }
   },
   "source": [
    "pip install datasets transformers"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (3.6.0)\r\n",
      "Requirement already satisfied: transformers in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (4.52.4)\r\n",
      "Requirement already satisfied: filelock in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (3.13.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (18.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (2.2.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (4.66.5)\r\n",
      "Requirement already satisfied: xxhash in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (0.30.1)\r\n",
      "Requirement already satisfied: packaging in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.11)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/juan/anaconda3/envs/CEIA/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aA-6FGao5Jih",
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.170346Z",
     "start_time": "2025-06-15T22:52:01.341230Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "#from transformers import MobileViTForImageClassification, MobileViTImageProcessor\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 19:52:05.082039: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-15 19:52:05.198994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750027925.244430 1430291 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750027925.257917 1430291 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750027925.356671 1430291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750027925.356684 1430291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750027925.356686 1430291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750027925.356687 1430291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-15 19:52:05.368266: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.340461Z",
     "start_time": "2025-06-15T22:52:07.204751Z"
    }
   },
   "source": [
    "device =  'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device) # Expected: ‘cuda’ if Linux else ‘mps’ if MacOS\n",
    "device =  'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.347531Z",
     "start_time": "2025-06-15T22:52:07.345929Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.__version__)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcyWg-Ii5AC6"
   },
   "source": [
    "# Descargando modelos"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fL7GJ_kQX0_g",
    "outputId": "a58c0f17-a7f2-44ac-8da8-05b59a97f26b",
    "ExecuteTime": {
     "end_time": "2025-06-15T23:54:23.845461Z",
     "start_time": "2025-06-15T23:54:23.841809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms\n",
    "\n",
    "img_dir_train = \"../data/raw/subset/train2014/train2014\"\n",
    "img_dir_val = \"../data/raw/subset/val2014\"\n",
    "img_dir_test = \"../data/raw/subset/test2014\"\n",
    "ann_coco_text = \"/home/juan/CEIA/CEIA-ViT/TrabajosPracticos/TP_Final/data/annotations/cocotext.v2.json\"\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# \n",
    "# coco_detection_train = CocoDetection(root=img_dir_train, annFile=ann_coco_text, transform=transform)\n",
    "# coco_detection_val = CocoDetection(root=img_dir_val, annFile=ann_coco_text, transform=transform)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsH_zu2ZX3ku",
    "outputId": "3c308d0d-66dd-4753-fd3b-64b73cac6dc6",
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.438975Z",
     "start_time": "2025-06-15T22:52:07.437657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from pycocotools.coco import COCO\n",
    "# \n",
    "# # Load annotations\n",
    "# coco_train = COCO(ann_file_train)\n",
    "# coco_test = COCO(ann_file_test)\n",
    "# coco_val = COCO(ann_file_val)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.483046Z",
     "start_time": "2025-06-15T22:52:07.481742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Get all image ids\n",
    "# img_ids_train = coco_detection_train.getImgIds()\n",
    "# img_data_train = coco_detection_train.loadImgs(img_ids_train[0])[0]\n",
    "# \n",
    "# img_ids_val = coco_detection_val.getImgIds()\n",
    "# img_data_val = coco_detection_val.loadImgs(img_ids_val[0])[0]\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.526663Z",
     "start_time": "2025-06-15T22:52:07.525240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Pick the first image in COCO-Train that has scene text\n",
    "# for img_id in coco_train.getImgIds():\n",
    "#     if img_id in filtered_img_train:\n",
    "#         selected_img_id = img_id\n",
    "#         break\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.570202Z",
     "start_time": "2025-06-15T22:52:07.568984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import json\n",
    "# \n",
    "# with open(ann_coco_text, 'r') as f:\n",
    "#     coco_text_data = json.load(f)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.613550Z",
     "start_time": "2025-06-15T22:52:07.612222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load COCO-Text annotations\n",
    "# scene_text_ann_ids = coco_text_data[\"imgToAnns\"].get(str(selected_img_id), [])\n",
    "# scene_text_anns = [coco_text_data[\"anns\"][str(ann_id)] for ann_id in scene_text_ann_ids]\n",
    "# coco_info = coco_train.loadImgs(selected_img_id)[0]\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:07.656621Z",
     "start_time": "2025-06-15T22:52:07.655331Z"
    }
   },
   "source": [
    "# train_dataset = load_dataset('cifar10', split='train[:4000]')\n",
    "# test_dataset = load_dataset('cifar10', split='test[:2000]')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dIi12TzP5Czj",
    "ExecuteTime": {
     "end_time": "2025-06-15T22:52:11.434696Z",
     "start_time": "2025-06-15T22:52:07.699542Z"
    }
   },
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "\n",
    "# 1. First fix the model configuration\n",
    "model.config.decoder_start_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:55:02.389325Z",
     "start_time": "2025-06-15T23:54:50.509146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class DonutTextDatasetFromCocoTextV2Raw(Dataset):\n",
    "    def __init__(self, image_dir, ann_file, processor, max_length=512):\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(ann_file, \"r\") as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        self.anns = coco_data[\"anns\"]\n",
    "        self.imgs = coco_data[\"imgs\"]\n",
    "\n",
    "        # Indexar anotaciones por imagen_id\n",
    "        self.ann_by_image = {}\n",
    "        for ann in self.anns.values():\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.ann_by_image:\n",
    "                self.ann_by_image[img_id] = []\n",
    "            self.ann_by_image[img_id].append(ann)\n",
    "\n",
    "        # Filtrar imágenes existentes en image_dir\n",
    "        image_files = set(os.listdir(image_dir))\n",
    "        self.valid_img_ids = [\n",
    "            int(img_id)\n",
    "            for img_id, img_data in self.imgs.items()\n",
    "            if img_data[\"file_name\"] in image_files\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.valid_img_ids[idx]\n",
    "        img_info = self.imgs[str(img_id)]\n",
    "        img_path = os.path.join(self.image_dir, img_info[\"file_name\"])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Obtener texto legible\n",
    "        anns = self.ann_by_image.get(img_id, [])\n",
    "        texts = [\n",
    "            ann[\"utf8_string\"] for ann in anns\n",
    "            if ann.get(\"legibility\", \"\") == \"legible\" and ann.get(\"utf8_string\", \"\").strip() != \"\"\n",
    "        ]\n",
    "        text = \" \".join(texts).strip()\n",
    "\n",
    "        # 1. Imagen\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\")[\"pixel_values\"].squeeze()\n",
    "        \n",
    "        # 2. Texto (tokenizado manualmente)\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.squeeze()\n",
    "\n",
    "        # 3. Reemplazo de tokens pad\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = DonutTextDatasetFromCocoTextV2Raw(img_dir_train, ann_coco_text, processor=processor)\n",
    "val_dataset = DonutTextDatasetFromCocoTextV2Raw(img_dir_val, ann_coco_text, processor=processor)\n",
    "test_dataset = DonutTextDatasetFromCocoTextV2Raw(img_dir_test, ann_coco_text, processor=processor)\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:55:04.897354Z",
     "start_time": "2025-06-15T23:55:04.895345Z"
    }
   },
   "cell_type": "code",
   "source": "val_dataset.__len__()\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7129"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:55:06.953858Z",
     "start_time": "2025-06-15T23:55:06.951344Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset.__len__()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15656"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:55:09.724185Z",
     "start_time": "2025-06-15T23:55:09.722010Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset.__len__()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:55:18.728107Z",
     "start_time": "2025-06-15T23:55:18.644544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = train_dataset[0]\n",
    "first_image = sample[\"pixel_values\"]  # This is the processed image tensor"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "first_text = processor.tokenizer.decode(sample[\"labels\"][train_dataset[0][\"labels\"] != -100])  # Decode the text labels",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "# If you want to see the actual PIL image before processing:\n",
    "first_image_path = os.path.join(img_dir_train, train_dataset.imgs[str(train_dataset.valid_img_ids[0])][\"file_name\"])\n",
    "original_image = Image.open(first_image_path).convert(\"RGB\")\n",
    "\n",
    "# Display results\n",
    "print(\"Detected Text:\", first_text)\n",
    "original_image.show()  # This will display the original image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inferencia con modelo preentrenado (sin finetuning)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prepare decoder inputs\n",
    "\n",
    "task_prompt = \"<s_cord-v2>\"\n",
    "\n",
    "decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "pixel_values = processor(original_image, return_tensors=\"pt\").pixel_values\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UJ5bx7fc5tuj"
   },
   "source": [
    "# Ensure model is on GPU\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Move inputs to GPU\n",
    "pixel_values = pixel_values.to('cuda')\n",
    "decoder_input_ids = decoder_input_ids.to('cuda')\n",
    "outputs = model.generate(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "result = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(result)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkuOvBPH5PPN"
   },
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IRTFO3Qx6o-H"
   },
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" # no utilizamos weights and biases"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T00:37:17.226010Z",
     "start_time": "2025-06-16T00:37:17.198566Z"
    }
   },
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./donut-finetuned-coco2014\",\n",
    "    per_device_train_batch_size=1,  # Minimum possible\n",
    "    gradient_accumulation_steps=8,  # No accumulation\n",
    "    fp16=True,                      # Mixed precision\n",
    "    gradient_checkpointing=True,    # Memory optimization\n",
    "    optim=\"adamw_8bit\",             # 8-bit optimizer\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_strategy=\"epoch\",             # Disable checkpoints\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1              # Start with 1 epoch\n",
    ")\n",
    "\n",
    "from jiwer import wer, cer\n",
    "\n",
    "def compute_metrics(pred,donut_processor):\n",
    "    # Extraer logits (shape: [batch, seq_len, vocab_size])\n",
    "    pred_ids = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Decodificar predicciones y etiquetas\n",
    "    pred_str = donut_processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = donut_processor.tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Limpiar cadenas (opcional, según formato de texto)\n",
    "    pred_str = [s.strip() for s in pred_str]\n",
    "    label_str = [s.strip() for s in label_str]\n",
    "\n",
    "    return {\n",
    "        \"cer\": cer(label_str, pred_str),\n",
    "        \"wer\": wer(label_str, pred_str)\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "id": "habLhitB4im7",
    "ExecuteTime": {
     "end_time": "2025-06-16T00:37:19.826227Z",
     "start_time": "2025-06-16T00:37:19.817253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "compute_metrics_bound = partial(compute_metrics, donut_processor=processor)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics_bound,\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Probamos evaluar el modelo tal cual viene"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T00:21:18.223591Z",
     "start_time": "2025-06-15T23:56:42.243554Z"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def manual_evaluate(model, dataset, processor, max_samples=None):\n",
    "    model.eval()\n",
    "    predictions, references = [], []\n",
    "    max_samples = max_samples or len(dataset)\n",
    "    empty_reference_count = 0\n",
    "    correct_empty_predictions = 0\n",
    "    \n",
    "    for i in tqdm(range(max_samples)):\n",
    "        try:\n",
    "            # 1. Load sample\n",
    "            sample = dataset[i]\n",
    "            pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(model.device)\n",
    "            \n",
    "            # 2. Generate prediction\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    pixel_values,\n",
    "                    max_length=512,\n",
    "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                    num_beams=1,\n",
    "                )\n",
    "            \n",
    "            # 3. Decode prediction\n",
    "            pred_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            \n",
    "            # 4. Safely decode labels\n",
    "            valid_label_ids = sample[\"labels\"][sample[\"labels\"] != -100]  # Remove padding\n",
    "            valid_label_ids = valid_label_ids[valid_label_ids < processor.tokenizer.vocab_size]  # Filter invalid IDs\n",
    "            true_text = processor.decode(valid_label_ids, skip_special_tokens=True)\n",
    "            # 4. Handle empty references separately\n",
    "            if not true_text:\n",
    "                empty_reference_count += 1\n",
    "                if not pred_text:\n",
    "                    correct_empty_predictions += 1\n",
    "                continue  # Skip jiwer calculation for empty references\n",
    "\n",
    "\n",
    "            predictions.append(pred_text.strip())\n",
    "            references.append(true_text.strip())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {i}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        # Periodic cleanup\n",
    "        if i % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate standard metrics (only non-empty references)\n",
    "    metrics = {\n",
    "        \"cer\": cer(references, predictions) if references else float('nan'),\n",
    "        \"wer\": wer(references, predictions) if references else float('nan'),\n",
    "        \"samples_evaluated\": len(references) + empty_reference_count,\n",
    "        \"empty_reference_accuracy\": correct_empty_predictions / empty_reference_count if empty_reference_count > 0 else float('nan'),\n",
    "        \"empty_references\": empty_reference_count,\n",
    "        \"correct_empty_predictions\": correct_empty_predictions\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "# Usage\n",
    "model.eval()  # Set to evaluation mode\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "metrics = manual_evaluate(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    processor=processor,\n",
    "    max_samples=None  # Start small, then increase if successful\n",
    ")\n",
    "print(metrics)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [24:35<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cer': 19.280644560357675, 'wer': 37.299666507860884, 'samples_evaluated': 700, 'empty_reference_accuracy': 0.38461538461538464, 'empty_references': 221, 'correct_empty_predictions': 85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T00:21:54.193287Z",
     "start_time": "2025-06-16T00:21:54.164323Z"
    }
   },
   "source": [
    "torch.cuda.empty_cache() # limpiamos cache"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Entrenamiento"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T00:34:33.492637Z",
     "start_time": "2025-06-16T00:34:27.247792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=True)  # Automatically finds latest checkpoint"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/15 21:34:28 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id e763370defc94af8bdcdad9366891636: Failed to log run data: Exception: Changing param values is not allowed. Param with key='per_device_train_batch_size' was already logged with value='1' for run ID='e763370defc94af8bdcdad9366891636'. Attempted logging new value '2'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/1957 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 37.38 MiB is free. Including non-PyTorch memory, this process has 7.53 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 270.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/transformers/trainer.py:2240\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2238\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2239\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[1;32m   2241\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   2242\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[1;32m   2243\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m   2244\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[1;32m   2245\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/transformers/trainer.py:2555\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2548\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2549\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2550\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2551\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2552\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2553\u001B[0m )\n\u001B[1;32m   2554\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2555\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n\u001B[1;32m   2557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2558\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2559\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2560\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2561\u001B[0m ):\n\u001B[1;32m   2562\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2563\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/transformers/trainer.py:3791\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m==\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED:\n\u001B[1;32m   3789\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscale_wrt_gas\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 3791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mbackward(loss, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   3793\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/accelerate/accelerator.py:2469\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2467\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   2468\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2469\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2470\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m learning_rate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_lomo_optimizer:\n\u001B[1;32m   2471\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    628\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[1;32m    348\u001B[0m     tensors,\n\u001B[1;32m    349\u001B[0m     grad_tensors_,\n\u001B[1;32m    350\u001B[0m     retain_graph,\n\u001B[1;32m    351\u001B[0m     create_graph,\n\u001B[1;32m    352\u001B[0m     inputs,\n\u001B[1;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    355\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    824\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    825\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/torch/autograd/function.py:307\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImplementing both \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackward\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvjp\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for a custom \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFunction is not allowed. You should only implement one \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    304\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof them.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    305\u001B[0m     )\n\u001B[1;32m    306\u001B[0m user_fn \u001B[38;5;241m=\u001B[39m vjp_fn \u001B[38;5;28;01mif\u001B[39;00m vjp_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Function\u001B[38;5;241m.\u001B[39mvjp \u001B[38;5;28;01melse\u001B[39;00m backward_fn\n\u001B[0;32m--> 307\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m user_fn(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs)\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/torch/utils/checkpoint.py:321\u001B[0m, in \u001B[0;36mCheckpointFunction.backward\u001B[0;34m(ctx, *args)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(outputs_with_grad) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    318\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnone of output has requires_grad=True,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m this checkpoint() is not necessary\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    320\u001B[0m     )\n\u001B[0;32m--> 321\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(outputs_with_grad, args_with_grad)\n\u001B[1;32m    322\u001B[0m grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\n\u001B[1;32m    323\u001B[0m     inp\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inp, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m inp \u001B[38;5;129;01min\u001B[39;00m detached_inputs\n\u001B[1;32m    325\u001B[0m )\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m+\u001B[39m grads\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[1;32m    348\u001B[0m     tensors,\n\u001B[1;32m    349\u001B[0m     grad_tensors_,\n\u001B[1;32m    350\u001B[0m     retain_graph,\n\u001B[1;32m    351\u001B[0m     create_graph,\n\u001B[1;32m    352\u001B[0m     inputs,\n\u001B[1;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    355\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/CEIA/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    824\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    825\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 37.38 MiB is free. Including non-PyTorch memory, this process has 7.53 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 270.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCLXJEtE9zro"
   },
   "source": "# Evaluamos modelo finetuneado para ver si mejoro alguna metrica"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import glob\n",
    "from transformers import VisionEncoderDecoderModel, DonutProcessor\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "checkpoints = glob.glob(\"./donut-finetuned-coco2014/checkpoint-*\")\n",
    "assert checkpoints, \"No checkpoints found!\"\n",
    "latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(latest_checkpoint)\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    latest_checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  \n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = DonutTextDatasetFromCocoTextV2Raw(\n",
    "    img_dir_test,  \n",
    "    ann_coco_text,\n",
    "    processor=processor,\n",
    "    max_length=512\n",
    ")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X6dQKURI7O04"
   },
   "source": [
    "metrics = manual_evaluate(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    processor=processor,\n",
    "    max_samples=None  \n",
    ")\n",
    "print(metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "052e6329b0534130812ba5ca45b083db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0abea91c811a4c3da3315f51db11d0cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ed6d36be6fa4797bf9b9b8c095b6475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3eeacb8cc59b407fb16b54fe1c883f5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fdbcc1df4bb4b3b8925a824e8088f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "437a1a92761e44bdb68e076ac3b3da9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a23a14bae46486dbe3bdc4a9cab1409": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "524ba3a721264edc9d081615489cd95d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_437a1a92761e44bdb68e076ac3b3da9b",
      "placeholder": "​",
      "style": "IPY_MODEL_0abea91c811a4c3da3315f51db11d0cd",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "5cd3b7ea4c774594bd10ad7e74d84d2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_524ba3a721264edc9d081615489cd95d",
       "IPY_MODEL_a03fc8049134429c8bfdad054fa4b9c7",
       "IPY_MODEL_f1430cbbd2ec4b7ba98e703a9e28b343"
      ],
      "layout": "IPY_MODEL_3eeacb8cc59b407fb16b54fe1c883f5a"
     }
    },
    "5ef7259729eb4a59bb392a920ecebc3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "636925ca49db4f9db22f0adf2271fb4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af807c2961134a6b84e103efecc1c338",
      "placeholder": "​",
      "style": "IPY_MODEL_85968d8cbd1a49ecbbe3a66b7a2506f9",
      "value": " 1000/1000 [00:29&lt;00:00, 250.78 examples/s]"
     }
    },
    "63c0fe8307d04343b09deb9028d92d6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac115587cbed4d95ba341da7974e3969",
       "IPY_MODEL_e1fd27c905a84959bac499d1b80f78d8",
       "IPY_MODEL_636925ca49db4f9db22f0adf2271fb4d"
      ],
      "layout": "IPY_MODEL_2ed6d36be6fa4797bf9b9b8c095b6475"
     }
    },
    "73652eb375bb40d59a4c30969dc9f550": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "741bac5ce3494d2fbfe6d4b241b1bb8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80153adaf2564a9081ab626832a77969": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85968d8cbd1a49ecbbe3a66b7a2506f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a03fc8049134429c8bfdad054fa4b9c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73652eb375bb40d59a4c30969dc9f550",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5ef7259729eb4a59bb392a920ecebc3b",
      "value": 2000
     }
    },
    "ac115587cbed4d95ba341da7974e3969": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_052e6329b0534130812ba5ca45b083db",
      "placeholder": "​",
      "style": "IPY_MODEL_4a23a14bae46486dbe3bdc4a9cab1409",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "af807c2961134a6b84e103efecc1c338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7d7acd6a9034b94ae5b6c18fd532020": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1fd27c905a84959bac499d1b80f78d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7d7acd6a9034b94ae5b6c18fd532020",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fdbcc1df4bb4b3b8925a824e8088f09",
      "value": 1000
     }
    },
    "f1430cbbd2ec4b7ba98e703a9e28b343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_741bac5ce3494d2fbfe6d4b241b1bb8c",
      "placeholder": "​",
      "style": "IPY_MODEL_80153adaf2564a9081ab626832a77969",
      "value": " 2000/2000 [00:35&lt;00:00,  4.71 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
