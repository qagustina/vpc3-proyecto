{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "C4hRg0ehJ-nc",
      "metadata": {
        "id": "C4hRg0ehJ-nc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "from transformers import (\n",
        "    VisionEncoderDecoderModel,\n",
        "    TrOCRProcessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2947fd42",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/agustindelavega/ai/vpc3-proyecto/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"../models/finetuned_trocr_weights\"\n",
        "processor = TrOCRProcessor.from_pretrained(model_name)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "\n",
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "QRstn09-cA1l",
      "metadata": {
        "id": "QRstn09-cA1l"
      },
      "outputs": [],
      "source": [
        "def levenshtein_distance(s1: str, s2: str) -> int:\n",
        "    \"\"\"Calculate the Levenshtein distance between two strings.\"\"\"\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def character_accuracy(predicted: str, ground_truth: str) -> float:\n",
        "    \"\"\"Calculate character-level accuracy.\"\"\"\n",
        "    if len(ground_truth) == 0:\n",
        "        return 1.0 if len(predicted) == 0 else 0.0\n",
        "    edit_distance = levenshtein_distance(predicted, ground_truth)\n",
        "    return 1.0 - (edit_distance / len(ground_truth)) # CER is normalized by GT length\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Normalize text for comparison (lowercase, strip whitespace).\"\"\"\n",
        "    return re.sub(r'\\s+', ' ', text.strip().lower())\n",
        "\n",
        "def evaluate_on_test_set(test_df, model, processor, device):\n",
        "    \"\"\"Evaluate the model on the test dataframe and return detailed results.\"\"\"\n",
        "    results = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\\nüöÄ Starting evaluation on {len(test_df)} test samples...\")\n",
        "\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating on test set\"):\n",
        "        image_path = os.path.join('/content/data/processed/val', row['image_path'])\n",
        "        ground_truth = str(row['text'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Could not find image {image_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(pixel_values)\n",
        "\n",
        "        prediction = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        pred_norm = normalize_text(prediction)\n",
        "        gt_norm = normalize_text(ground_truth)\n",
        "\n",
        "        char_acc = character_accuracy(pred_norm, gt_norm)\n",
        "\n",
        "        results.append({\n",
        "            'image_path': image_path,\n",
        "            'ground_truth': ground_truth,\n",
        "            'prediction': prediction,\n",
        "            'char_accuracy': char_acc,\n",
        "            'edit_distance': levenshtein_distance(pred_norm, gt_norm),\n",
        "            'gt_length': len(gt_norm)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "lqhYsGbVcJBj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqhYsGbVcJBj",
        "outputId": "dab3ead2-d190-4a59-cae9-8061f2c7e63f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Starting evaluation on 20809 test samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating on test set:  12%|‚ñà‚ñè        | 2442/20809 [02:54<20:20, 15.05it/s]The channel dimension is ambiguous. Got image shape (3, 8, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  26%|‚ñà‚ñà‚ñå       | 5456/20809 [06:26<17:25, 14.69it/s]The channel dimension is ambiguous. Got image shape (3, 20, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  30%|‚ñà‚ñà‚ñà       | 6324/20809 [07:28<20:06, 12.01it/s]The channel dimension is ambiguous. Got image shape (3, 5, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  35%|‚ñà‚ñà‚ñà‚ñç      | 7239/20809 [08:32<17:31, 12.91it/s]The channel dimension is ambiguous. Got image shape (3, 12, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  37%|‚ñà‚ñà‚ñà‚ñã      | 7631/20809 [08:59<19:08, 11.48it/s]The channel dimension is ambiguous. Got image shape (3, 9, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 8780/20809 [10:20<14:37, 13.72it/s]The channel dimension is ambiguous. Got image shape (3, 6, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 16028/20809 [18:58<06:22, 12.48it/s]The channel dimension is ambiguous. Got image shape (3, 17, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16661/20809 [19:41<05:01, 13.78it/s]The channel dimension is ambiguous. Got image shape (3, 17, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16686/20809 [19:43<04:58, 13.81it/s]The channel dimension is ambiguous. Got image shape (3, 15, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 19082/20809 [22:34<02:00, 14.31it/s]The channel dimension is ambiguous. Got image shape (3, 5, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n",
            "Evaluating on test set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20809/20809 [24:38<00:00, 14.07it/s]\n"
          ]
        }
      ],
      "source": [
        "test_df = pd.read_csv(os.path.join('/content/data/processed/val', 'labels.csv'))\n",
        "\n",
        "test_df.columns = ['image_path', 'text']\n",
        "evaluation_results_df = evaluate_on_test_set(test_df, model, processor, \"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "B1lqROl-4zID",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1lqROl-4zID",
        "outputId": "62d6af0e-9efa-49fe-d6be-65b44d2ca521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Generating Evaluation Summary...\n",
            "===================================\n",
            "\n",
            "--- Top-Level Metrics ---\n",
            "Total Samples Evaluated: 20809\n",
            "‚úÖ Mean Character Accuracy: 0.6717  (Higher is better)\n",
            "‚ùå Mean Character Error Rate (CER): 0.3283  (Lower is better)\n",
            "üéØ Exact Match Rate: 0.4726 (9835/20809 perfect predictions)\n",
            "\n",
            "--- Distribution of Character Accuracy ---\n",
            "count    20809.000000\n",
            "mean         0.671731\n",
            "std          0.442648\n",
            "min        -11.400000\n",
            "25%          0.400000\n",
            "50%          0.857143\n",
            "75%          1.000000\n",
            "max          1.000000\n",
            "Name: char_accuracy, dtype: float64\n",
            "\n",
            "--- Worst Performing Examples (Lowest Accuracy) ---\n",
            "      ground_truth  \\\n",
            "13590        kuchy   \n",
            "18849       OWDOWN   \n",
            "7213      MOTOROLA   \n",
            "2765     FOKEIGNES   \n",
            "20048          JGS   \n",
            "\n",
            "                                                                        prediction  \\\n",
            "13590             EXCLUDING ON ONLY ON ONLY ONCLUSED ON FACE NOT RECE ON RECE REIN   \n",
            "18849       AMPROVING ON ONLY ON ONLY ON ONLY ON ONLY ON ON ON ON ON ONLY ON ON ON   \n",
            "7213      CASHIER ONCLOSED ON ONLY ON FACE ON FACE ON FACE ON FACE ON FACE ON FACE   \n",
            "2765   TOTAL ONCLUSIVE ON FACE ON FACEBOOK ON FACEBOOK ON FACE ON FACEBOOK ON FACE   \n",
            "20048                                                        WWW.J.G.S.CONCEPLSCOM   \n",
            "\n",
            "       char_accuracy        cer  \n",
            "13590     -11.400000  12.400000  \n",
            "18849     -10.166667  11.166667  \n",
            "7213       -7.375000   8.375000  \n",
            "2765       -6.666667   7.666667  \n",
            "20048      -5.000000   6.000000  \n",
            "\n",
            "--- Best Performing Examples (Highest Accuracy) ---\n",
            "   ground_truth prediction  char_accuracy  cer\n",
            "53           40         40            1.0  0.0\n",
            "54      PARKING    PARKING            1.0  0.0\n",
            "42         SKOL       SKOL            1.0  0.0\n",
            "46     CLAGGING   CLAGGING            1.0  0.0\n",
            "1       Dealers    DEALERS            1.0  0.0\n"
          ]
        }
      ],
      "source": [
        "print(\"üìä Generating Evaluation Summary...\")\n",
        "print(\"===================================\")\n",
        "\n",
        "# Calculate Character Error Rate (CER)\n",
        "# CER = Edit Distance / Length of Ground Truth\n",
        "evaluation_results_df['cer'] = evaluation_results_df['edit_distance'] / evaluation_results_df['gt_length']\n",
        "evaluation_results_df['cer'] = evaluation_results_df['cer'].fillna(0)\n",
        "\n",
        "\n",
        "mean_char_accuracy = evaluation_results_df['char_accuracy'].mean()\n",
        "mean_cer = evaluation_results_df['cer'].mean()\n",
        "\n",
        "# Calculate Exact Match Rate (where edit distance is 0)\n",
        "exact_matches = (evaluation_results_df['edit_distance'] == 0).sum()\n",
        "total_samples = len(evaluation_results_df)\n",
        "exact_match_rate = exact_matches / total_samples\n",
        "\n",
        "print(\"\\n--- Top-Level Metrics ---\")\n",
        "print(f\"Total Samples Evaluated: {total_samples}\")\n",
        "print(f\"‚úÖ Mean Character Accuracy: {mean_char_accuracy:.4f}  (Higher is better)\")\n",
        "print(f\"‚ùå Mean Character Error Rate (CER): {mean_cer:.4f}  (Lower is better)\")\n",
        "print(f\"üéØ Exact Match Rate: {exact_match_rate:.4f} ({exact_matches}/{total_samples} perfect predictions)\")\n",
        "\n",
        "\n",
        "# --- 2. Distribution of Scores ---\n",
        "print(\"\\n--- Distribution of Character Accuracy ---\")\n",
        "print(evaluation_results_df['char_accuracy'].describe())\n",
        "\n",
        "\n",
        "# --- 3. Qualitative Analysis (Worst & Best Cases) ---\n",
        "\n",
        "results_sorted = evaluation_results_df.sort_values(by='char_accuracy', ascending=True)\n",
        "\n",
        "print(\"\\n--- Worst Performing Examples (Lowest Accuracy) ---\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(results_sorted[['ground_truth', 'prediction', 'char_accuracy', 'cer']].head(5))\n",
        "\n",
        "\n",
        "print(\"\\n--- Best Performing Examples (Highest Accuracy) ---\")\n",
        "print(results_sorted[['ground_truth', 'prediction', 'char_accuracy', 'cer']].tail(5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
