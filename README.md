#  Trabajo Final - Visi√≥n por Computadora III (CEIA)

###  Integrantes

- Florentino Arias  
- Juan Cruz Pi√±ero  
- Agustina Quiros  
- Agust√≠n de la Vega  

### Traducci√≥n de Im√°genes a Texto 
Aplicaci√≥n de modelos de OCR sobre el dataset COCO-Text.
Este proyecto explora la capacidad de modelos de OCR basados en transformers para transcribir texto presente en im√°genes naturales. Se comparan dos enfoques:  
- **TrOCR**, orientado a la transcripci√≥n directa de texto en regiones espec√≠ficas.  
- **Donut**, dise√±ado para el entendimiento estructurado de documentos completos.

Se trabaj√≥ con el dataset **COCO-Text**, realizando un proceso de *fine-tuning* y evaluaci√≥n basado en m√©tricas como **Mean Character Accuracy** y **Character Error Rate**.

#### üñºÔ∏èDataset: [Coco-Text dataset](https://bgshih.github.io/cocotext/)

#### ü§ó Modelos Utilizados

- [TrOCR](https://huggingface.co/microsoft/trocr-base-handwritten)

- [Donut](https://huggingface.co/naver-clova-ix/donut-base)



<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>




## Project Organization

```
‚îú‚îÄ‚îÄ LICENSE            <- Open-source license if one is chosen
‚îú‚îÄ‚îÄ Makefile           <- Makefile with convenience commands like `make data` or `make train`
‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
‚îÇ   ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
‚îÇ   ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
‚îÇ   ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
‚îÇ
‚îú‚îÄ‚îÄ docs               <- A default mkdocs project; see www.mkdocs.org for details
‚îÇ
‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
‚îÇ
‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
‚îÇ                         `1.0-jqp-initial-data-exploration`.
‚îÇ
‚îú‚îÄ‚îÄ pyproject.toml     <- Project configuration file with package metadata for 
‚îÇ                         vpc3-proyecto and configuration for tools like black
‚îÇ
‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
‚îÇ
‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
‚îÇ   ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
‚îÇ                         generated with `pip freeze > requirements.txt`
‚îÇ
‚îú‚îÄ‚îÄ setup.cfg          <- Configuration file for flake8
‚îÇ
‚îî‚îÄ‚îÄ vpc3-proyecto   <- Source code for use in this project.
    ‚îÇ
    ‚îú‚îÄ‚îÄ __init__.py             <- Makes vpc3-proyecto a Python module
    ‚îÇ
    ‚îú‚îÄ‚îÄ config.py               <- Store useful variables and configuration
    ‚îÇ
    ‚îú‚îÄ‚îÄ dataset.py              <- Scripts to download or generate data
    ‚îÇ
    ‚îú‚îÄ‚îÄ features.py             <- Code to create features for modeling
    ‚îÇ
    ‚îú‚îÄ‚îÄ modeling                
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py 
    ‚îÇ   ‚îú‚îÄ‚îÄ predict.py          <- Code to run model inference with trained models          
    ‚îÇ   ‚îî‚îÄ‚îÄ train.py            <- Code to train models
    ‚îÇ
    ‚îî‚îÄ‚îÄ plots.py                <- Code to create visualizations
```

## Data preprocessing

### üñ•Ô∏è M√©todo 1: Notebook `aq-data-processing.ipynb`

#### Pasos de Ejecuci√≥n
1. Abrir el notebook en Jupyter:
2. Modificar la variable requerida para leer la ubicaci√≥n de los archivos zip del dataset cocotext.
3. ejecutar la notebook
4. se generaran carpetas en la carpeta `data` (relativa a la ubicaci√≥n del notebook) con los archivos procesados.

### üñ•Ô∏è M√©todo 2: Desde subm√≥dulo vp3_proyecto ejecutar notebook `data/dataset_split.ipynb`

#### Pasos de Ejecuci√≥n
1. Setear variables de entorno en archivo .env en la raiz del subm√≥dulo (vpc3-proyecto)
2. ejecutar notebook `data/dataset_split.ipynb`
3. se generaran carpetas en la carpeta indicada en la variable de entorno `PROCESSED_DATA_DIR` con las imagenes divididas en subdirectorios segun la proporci√≥n indicada.
--------

## Fine tuning

### 1. Setup archivo `.env`
Modificar un archivo `.env` en el directorio ra√≠z del proyecto con este contenido:

```ini
PROCESSED_DATA_DIR=/ruta/absoluta/a/data/processed ==> ruta donde se quiere generar el split del dataset
RAW_DATA_DIR=/ruta/absoluta/a/data/raw ==> ruta a .zip donde estan las imagenes y las anottations
CHECKPOINT_DIR=/ruta/absoluta/a/checkpoints ==> carpeta de guardado de modelos (ej: /home/juan/CEIA/vpc3_proyecto/models)
```
### Modificar configuracion de par√°metros de entrenamiento seg√∫n necesidad.

### Ejecutar notebook correspondiente al modelo deseado

Por ejemplo, para donut, se debe ejecutar la notebook 'vpc3_proyecto/model_training/donut_fine_tuning.ipynb'

El modelo resultante ser√° guardado en el directorio indicado.

## Evaluaci√≥n de modelo

Para esto se pueden utilizar las notebooks dentro de 'vpc3_proyecto/model_evaluation/'

Es necesario tener las variables de entorno configuradas en el archivo `.env` correspondiente.

### Ejecutar notebook correspondiente al modelo deseado

Por ejemplo, para donut, se debe ejecutar la notebook 'vpc3_proyecto/model_evaluation/donut_evaluation.ipynb'

Es posible que sea necesario modificar la notebook para indicar el directorio especifico en el cual se encuentra el modelo guardado.

### Visualizaci√≥n de atenci√≥n

