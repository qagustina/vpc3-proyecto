{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## first we  load the dotenv to read the root directory for all teh ttrainings \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(\"..\",\".env\"))\n",
    "#%"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "checkpoint_dir = os.getenv(\"CHECKPOINT_DIR\")",
   "id": "672060867b7513e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#now we inspect all the files called metrics.json inside the checkpoint dir \n",
    "import glob\n",
    "files = glob.glob(checkpoint_dir + \"/*metrics.json\")\n",
    "# now we construct a pandas dataframe to associate each metrics.json with its model (the name of the model is the folder name immediately after the checkpoint dir inside which the metrics.jsopn was found)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df[\"model\"] = [os.path.basename(os.path.dirname(file)) for file in files]\n",
    "df[\"metrics.json\"] = files\n",
    "df\n",
    "\n"
   ],
   "id": "31abe6ea6f03b949"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Ejemplo de Uso ---\n",
    "\n",
    "# Escenario 1: Comparar métricas de diferentes ejecuciones (cada una en su propia carpeta de resultados)\n",
    "# Suponiendo que tienes resultados en estas rutas:\n",
    "# ./results_model_A/metrics.json\n",
    "# ./results_model_B/metrics.json\n",
    "# ./results_model_C/metrics.json\n",
    "\n",
    "# json_paths_example_1 = [\n",
    "#     \"path/to/your/results_model_A/metrics.json\",\n",
    "#     \"path/to/your/results_model_B/metrics.json\",\n",
    "#     \"path/to/your/results_model_C/metrics.json\",\n",
    "# ]\n",
    "# plot_comparison_metrics(json_paths_example_1)\n",
    "\n",
    "\n",
    "# Escenario 2: Comparar métricas de diferentes checkpoints dentro de una misma ejecución\n",
    "# Suponiendo que tu Trainer guarda checkpoints así:\n",
    "# ./my_training_output/checkpoint-100/metrics.json\n",
    "# ./my_training_output/checkpoint-200/metrics.json\n",
    "# ./my_training_output/checkpoint-300/metrics.json\n",
    "\n",
    "# json_paths_example_2 = [\n",
    "#     \"path/to/your/my_training_output/checkpoint-100\", # La función buscará 'metrics.json' dentro\n",
    "#     \"path/to/your/my_training_output/checkpoint-200\",\n",
    "#     \"path/to/your/my_training_output/checkpoint-300\",\n",
    "# ]\n",
    "# plot_comparison_metrics(json_paths_example_2, title_prefix=\"Progreso del Entrenamiento\")\n",
    "\n",
    "\n",
    "# Escenario 3: Combinar ambos escenarios, nombrando las ejecuciones manualmente\n",
    "# json_paths_example_3 = [\n",
    "#     {\"name\": \"Modelo A (Final)\", \"path\": \"path/to/your/results_model_A/metrics.json\"},\n",
    "#     {\"name\": \"Modelo B (Checkpoint 500)\", \"path\": \"path/to/your/model_B_output/checkpoint-500/metrics.json\"},\n",
    "# ]\n",
    "# plot_comparison_metrics(\n",
    "#     [item[\"path\"] for item in json_paths_example_3], # Pasa solo las rutas\n",
    "#     title_prefix=\"Comparativa Específica\",\n",
    "#     # Aquí tendrías que mapear manualmente los nombres para que aparezcan en el gráfico si son complejos.\n",
    "#     # La implementación actual usa el nombre de la carpeta o un \"Run X\".\n",
    "#     # Si necesitas nombres personalizados, el diccionario 'all_metrics_data' debería crearse con esos nombres.\n",
    "# )\n",
    "\n",
    "\n",
    "# --- Consideraciones Adicionales ---\n",
    "\n",
    "# Para que la función funcione correctamente, debes asegurarte de que:\n",
    "# 1. Los archivos 'metrics.json' existan en las rutas que le pasas.\n",
    "# 2. El contenido de 'metrics.json' sea un diccionario que contenga las métricas 'cer' y 'wer' (y las otras si quieres graficarlas,\n",
    "#    añadiéndolas a la lista `metric_names`). Recuerda que `manual_evaluate` guarda las métricas bajo la clave \"metrics\" si\n",
    "#    `results_save_path` está definido, así que la función `plot_comparison_metrics` lo maneja.\n",
    "\n",
    "# Si tus JSON tienen una estructura diferente (ej., las métricas no están bajo una clave 'metrics'),\n",
    "# tendrías que ajustar la lógica de carga dentro de `plot_comparison_metrics`."
   ],
   "id": "96b704660c658446"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
